{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run MobileNet on a Camera \n",
    "\n",
    "* [ssdlite320_mobilenet_v3_large](https://pytorch.org/vision/stable/models/generated/torchvision.models.detection.ssdlite320_mobilenet_v3_large.html)\n",
    "* [Real Time Inference on Raspberry Pi 4](https://pytorch.org/tutorials/intermediate/realtime_rpi.html)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "if \"..\" not in sys.path:\n",
    "    sys.path.append(\"..\")  # Append pieye module root to sys.path\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torchvision import models, transforms\n",
    "\n",
    "# from pieye.video import play\n",
    "from pieye.utils import show_bboxes\n",
    "\n",
    "\n",
    "# Note, The aarch64 version of pytorch requires using the `qnnpack` engine.\n",
    "torch.backends.quantized.engine = \"qnnpack\"\n",
    "# torch.set_num_threads(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ WARN:0@161.299] global cap_v4l.cpp:999 open VIDEOIO(V4L2:/dev/video0): can't open camera by index\n",
      "[ERROR:0@161.299] global obsensor_uvc_stream_channel.cpp:158 getStreamChannelGroup Camera index out of range\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "< cv2.VideoCapture 0x7f5c3bc3d6b0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video = cv2.VideoCapture(0)\n",
    "video.set(cv2.CAP_PROP_FRAME_WIDTH, 320)\n",
    "video.set(cv2.CAP_PROP_FRAME_HEIGHT, 320)\n",
    "video.set(cv2.CAP_PROP_FPS, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, frame = video.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.fromarray(frame[:, :, [2, 1, 0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 320, 320])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess = transforms.Compose([\n",
    "    # convert the frame to a CHW torch tensor for training\n",
    "    transforms.ToTensor(),\n",
    "    # normalize the colors to the range that mobilenet_v2/3 expect\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/junkimin/miniconda3/envs/pi-eye/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/junkimin/miniconda3/envs/pi-eye/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V3_Large_QuantizedWeights.IMAGENET1K_QNNPACK_V1`. You can also use `weights=MobileNet_V3_Large_QuantizedWeights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/quantized/mobilenet_v3_large_qnnpack-5bcacf28.pth\" to /home/junkimin/.cache/torch/hub/checkpoints/mobilenet_v3_large_qnnpack-5bcacf28.pth\n",
      "100.0%\n",
      "/home/junkimin/miniconda3/envs/pi-eye/lib/python3.10/site-packages/torch/ao/quantization/utils.py:376: UserWarning: must run observer before calling calculate_qparams. Returning default values.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Quantized (int8) model\n",
    "# Quantized MobileNetV3-Large is about the same speed and size as MobileNetv3-Small, but it has higher accuracy.\n",
    "model = models.quantization.mobilenet_v3_large(pretrained=True, quantize=True)\n",
    "\n",
    "# Convert into jit (graph mode). In eager mode, operators in a model are immediately executed as they are encountered. In contrast, in graph mode, operators are first synthesized into a graph, which will then be compiled and executed as a whole\n",
    "# This reduce overhead\n",
    "model = torch.jit.script(model)\n",
    "\n",
    "# Let batchnorm or dropout layers work in eval mode instead of training mode.\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor = preprocess(image)\n",
    "# The model can handle multiple images simultaneously so we need to add an\n",
    "# empty dimension for the batch.\n",
    "# [320, 320, 3] -> [1, 3, 320, 320]\n",
    "input_batch = input_tensor.unsqueeze(0)\n",
    "input_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deactivate autograd engine to not backprop\n",
    "with torch.no_grad():\n",
    "    output = model(input_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1000])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"imagenet_class_labels.txt\") as f:\n",
    "    idx_to_label = eval(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.78% balloon\n",
      "11.78% nipple\n",
      "9.80% candle, taper, wax light\n",
      "9.80% ping-pong ball\n",
      "6.77% hair spray\n",
      "6.77% water bottle\n",
      "5.63% lighter, light, igniter, ignitor\n",
      "5.63% sunscreen, sunblock, sun blocker\n",
      "5.63% ice lolly, lolly, lollipop, popsicle\n",
      "3.24% cocktail shaker\n"
     ]
    }
   ],
   "source": [
    "display(image)\n",
    "\n",
    "top = list(enumerate(output[0].softmax(dim=0)))\n",
    "top.sort(key=lambda x: x[1], reverse=True)\n",
    "for idx, val in top[:10]:\n",
    "    print(f\"{val.item()*100:.2f}% {idx_to_label[idx]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Object Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detection_threshold = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_no_normalize = transforms.Compose([\n",
    "    # convert the frame to a CHW torch tensor for training\n",
    "    transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/junkimin/miniconda3/envs/pi-eye/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/junkimin/miniconda3/envs/pi-eye/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=SSDLite320_MobileNet_V3_Large_Weights.COCO_V1`. You can also use `weights=SSDLite320_MobileNet_V3_Large_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/ssdlite320_mobilenet_v3_large_coco-a79551df.pth\" to /home/junkimin/.cache/torch/hub/checkpoints/ssdlite320_mobilenet_v3_large_coco-a79551df.pth\n",
      "100.0%\n"
     ]
    }
   ],
   "source": [
    "# Load object detection model. TODO: Need to quantize manually\n",
    "model = models.detection.ssdlite320_mobilenet_v3_large(pretrained=True)\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor = preprocess_no_normalize(image)\n",
    "input_batch = input_tensor.unsqueeze(0)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_bboxes(image=input_tensor, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video.release()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
